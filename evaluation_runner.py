# evaluation_runner.py
import os
import sys
sys.path.append('src')

from src.ML_TEMPERATURE_PREDICTION.config.configuration import ConfigurationManager
from src.ML_TEMPERATURE_PREDICTION.components.model_evaluation import ModelEvaluation
from src.ML_TEMPERATURE_PREDICTION.logging import logger

def run_enhanced_evaluation():
    """
    Run enhanced evaluation with temperature analysis
    """
    try:
        # Initialize configuration
        config = ConfigurationManager()
        model_eval_config = config.get_model_evaluation_config()
        
        # Initialize model evaluation
        model_evaluation = ModelEvaluation(config=model_eval_config)
        
        # Load test data (you'll need to implement this based on your data loading)
        # test_loader = load_test_data()  # Implement this function
        
        # For demonstration, let's simulate the evaluation with your current results
        logger.info("Running enhanced evaluation with temperature analysis...")
        
        # Your current evaluation results
        current_results = {
            'l1_loss': 0.18755212832580914,
            'psnr': 18.895721599477806,
            'ssim': 0.6028149900483882
        }
        
        # Simulate temperature metrics (you would get these from actual evaluation)
        simulated_temp_metrics = {
            'mae': 0.75,  # Mean Absolute Error in ¬∞C
            'rmse': 1.2,  # Root Mean Square Error in ¬∞C
            'r2': 0.65,   # R-squared coefficient
            'temp_range_compliance': 87.5,  # Percentage within 28-30¬∞C range
            'mean_predicted_temp': 29.1,
            'mean_target_temp': 29.0,
            'temp_std_predicted': 0.8,
            'temp_std_target': 0.7
        }
        
        # Combine metrics
        all_metrics = {**current_results, **simulated_temp_metrics}
        
        # Create evaluation directory
        eval_dir = "artifacts/model_evaluation/results"
        os.makedirs(eval_dir, exist_ok=True)
        os.makedirs(os.path.join(eval_dir, 'plots'), exist_ok=True)
        
        # Create metrics summary plot
        metrics_plot_path = os.path.join(eval_dir, 'plots', 'metrics_summary.png')
        model_evaluation.create_metrics_summary_plot(all_metrics, metrics_plot_path)
        # Create temperature analysis if we have temperature data
        if 'mae' in all_metrics:
            # Simulate temperature data for visualization (in real scenario, this comes from actual evaluation)
            import numpy as np
            
            # Generate simulated temperature data for demonstration
            np.random.seed(42)  # For reproducible results
            n_samples = 10000
            
            # Simulate target temperatures (ground truth) around 28-30¬∞C
            target_temps = np.random.normal(29.0, 0.7, n_samples)
            target_temps = np.clip(target_temps, 28.0, 30.0)
            
            # Simulate predicted temperatures with some error
            noise = np.random.normal(0, 0.75, n_samples)  # MAE ‚âà 0.75
            predicted_temps = target_temps + noise
            
            # Create temperature visualization
            temp_plot_path = os.path.join(eval_dir, 'plots', 'temperature_analysis.png')
            model_evaluation.create_temperature_visualizations(
                predicted_temps, target_temps, temp_plot_path
            )
        
        # Save comprehensive metrics to JSON
        import json
        with open(os.path.join(eval_dir, 'enhanced_metrics.json'), 'w') as f:
            json.dump(all_metrics, f, indent=4)
        
        # Print evaluation summary
        print("\n" + "="*60)
        print("ENHANCED MODEL EVALUATION RESULTS")
        print("="*60)
        print(f"üìä Image Quality Metrics:")
        print(f"   L1 Loss: {current_results['l1_loss']:.4f} (Target: ‚â§ 0.15)")
        print(f"   PSNR: {current_results['psnr']:.2f} dB (Target: ‚â• 25 dB)")
        print(f"   SSIM: {current_results['ssim']:.4f} (Target: ‚â• 0.75)")
        
        print(f"\nüå°Ô∏è  Temperature Prediction Metrics:")
        print(f"   MAE: {simulated_temp_metrics['mae']:.2f}¬∞C (Target: ‚â§ 0.5¬∞C)")
        print(f"   RMSE: {simulated_temp_metrics['rmse']:.2f}¬∞C (Target: ‚â§ 1.0¬∞C)")
        print(f"   R¬≤: {simulated_temp_metrics['r2']:.3f} (Target: ‚â• 0.8)")
        print(f"   Range Compliance: {simulated_temp_metrics['temp_range_compliance']:.1f}% (Target: ‚â• 95%)")
        
        print(f"\nüìà Performance Status:")
        # Check if metrics meet targets
        status_checks = [
            ("L1 Loss", current_results['l1_loss'] <= 0.15),
            ("PSNR", current_results['psnr'] >= 25.0),
            ("SSIM", current_results['ssim'] >= 0.75),
            ("MAE", simulated_temp_metrics['mae'] <= 0.5),
            ("RMSE", simulated_temp_metrics['rmse'] <= 1.0),
            ("R¬≤", simulated_temp_metrics['r2'] >= 0.8),
            ("Range Compliance", simulated_temp_metrics['temp_range_compliance'] >= 95.0)
        ]
        
        passed = sum(1 for _, status in status_checks if status)
        total = len(status_checks)
        
        for metric, status in status_checks:
            status_icon = "‚úÖ" if status else "‚ùå"
            print(f"   {status_icon} {metric}")
        
        print(f"\nüéØ Overall Score: {passed}/{total} metrics passed ({passed/total*100:.1f}%)")
        
        # Recommendations based on results
        print(f"\nüí° Recommendations:")
        if current_results['l1_loss'] > 0.15:
            print("   ‚Ä¢ L1 Loss is high - consider increasing training epochs or adjusting learning rate")
        if current_results['psnr'] < 25.0:
            print("   ‚Ä¢ PSNR is low - model needs improvement in reconstruction quality")
        if current_results['ssim'] < 0.75:
            print("   ‚Ä¢ SSIM is low - focus on preserving structural information in training")
        if simulated_temp_metrics['mae'] > 0.5:
            print("   ‚Ä¢ Temperature MAE is high - consider temperature-specific loss functions")
        if simulated_temp_metrics['rmse'] > 1.0:
            print("   ‚Ä¢ Temperature RMSE is high - review temperature calibration process")
        if simulated_temp_metrics['r2'] < 0.8:
            print("   ‚Ä¢ Low correlation - model may need more diverse training data")
        if simulated_temp_metrics['temp_range_compliance'] < 95.0:
            print("   ‚Ä¢ Temperature range compliance is low - add constraints during training")
        
        print(f"\nüìÅ Results saved to: {eval_dir}")
        print("="*60)
        
        return all_metrics
        
    except Exception as e:
        logger.error(f"Evaluation failed: {str(e)}")
        raise e

if __name__ == "__main__":
    run_enhanced_evaluation()